{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T22:57:21.893944Z","iopub.execute_input":"2021-12-05T22:57:21.894646Z","iopub.status.idle":"2021-12-05T22:57:27.464565Z","shell.execute_reply.started":"2021-12-05T22:57:21.894538Z","shell.execute_reply":"2021-12-05T22:57:27.463622Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, LeakyReLU, Dropout, BatchNormalization,Flatten,Conv2D,Reshape,MaxPool2D, Conv2DTranspose,BatchNormalization\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.applications import VGG16\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sys, os\nimport random\nimport cv2\npathname=\"../input/vggface-using-tripletloss/celebs/celebs\"\ndirList=os.listdir(pathname)\n\nln=len(dirList)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T23:38:54.673490Z","iopub.execute_input":"2021-12-05T23:38:54.674246Z","iopub.status.idle":"2021-12-05T23:38:54.684197Z","shell.execute_reply.started":"2021-12-05T23:38:54.674200Z","shell.execute_reply":"2021-12-05T23:38:54.683463Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Load in the data\n#mnist = tf.keras.datasets.mnist\n\n#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# map inputs to (-1, +1) for better training\n#x_train, x_test = x_train / 255.0 * 2 - 1, x_test / 255.0 * 2 - 1\n#print(\"x_train.shape:\", x_train.shape)\n#print(\"x_test.shape:\", x_test.shape)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-12-05T23:34:53.369576Z","iopub.execute_input":"2021-12-05T23:34:53.369790Z","iopub.status.idle":"2021-12-05T23:34:53.374471Z","shell.execute_reply.started":"2021-12-05T23:34:53.369764Z","shell.execute_reply":"2021-12-05T23:34:53.373695Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#x_train=np.reshape(x_train,(60000,28,28,1))\n#x_test=np.reshape(x_test,(10000,28,28,1))","metadata":{"execution":{"iopub.status.busy":"2021-12-05T23:34:53.375769Z","iopub.execute_input":"2021-12-05T23:34:53.376244Z","iopub.status.idle":"2021-12-05T23:34:53.388712Z","shell.execute_reply.started":"2021-12-05T23:34:53.376204Z","shell.execute_reply":"2021-12-05T23:34:53.387881Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Get the generator model\ndef build_generator():\n    \n    generator=Sequential()\n    generator.add(Conv2DTranspose(512, (4, 4),input_shape=(28,28,512), padding=\"same\",activation=LeakyReLU()))\n    generator.add(Conv2DTranspose(512, (4, 4),input_shape=(28,28,512), padding=\"same\",activation=LeakyReLU()))\n    \n    #generator.add(BatchNormalization())\n    \n    #generator.add(Conv2DTranspose(512, (6, 6), strides=2, padding=\"same\", activation=LeakyReLU()))\n    #generator.add(Conv2DTranspose(512, (6, 6), padding=\"same\", activation=LeakyReLU()))\n    \n    generator.add(BatchNormalization())\n    generator.add(Conv2DTranspose(256, (8, 8), strides=2, padding=\"same\", activation=LeakyReLU()))\n    generator.add(Conv2DTranspose(256, (8, 8),  padding=\"same\", activation=LeakyReLU()))\n\n\n    generator.add(BatchNormalization())\n\n    generator.add(Conv2DTranspose(128, (10, 10), strides=2, padding=\"same\", activation=LeakyReLU()))\n    generator.add(Conv2DTranspose(128, (10, 10),  padding=\"same\", activation=LeakyReLU()))\n\n\n    generator.add(BatchNormalization())\n    generator.add(Conv2DTranspose(64, (12, 12), strides=2, padding=\"same\", activation=LeakyReLU()))\n    generator.add(Conv2DTranspose(filters=64, kernel_size=(12,12), padding=\"same\", activation=LeakyReLU()))\n    generator.add(Conv2DTranspose(filters=64, kernel_size=(12,12), padding=\"same\", activation=LeakyReLU()))\n\n    generator.add(BatchNormalization())\n    generator.add(Conv2DTranspose(filters=3,kernel_size=(12, 12), padding=\"same\",activation=LeakyReLU()))\n    generator.add(Conv2DTranspose(filters=3,kernel_size=(12, 12), padding=\"same\",activation=LeakyReLU()))\n    generator.add(Conv2DTranspose(filters=3,kernel_size=(12, 12), padding=\"same\",activation=LeakyReLU()))\n    #generator.summary()\n    return generator","metadata":{"execution":{"iopub.status.busy":"2021-12-05T23:35:12.701017Z","iopub.execute_input":"2021-12-05T23:35:12.701286Z","iopub.status.idle":"2021-12-05T23:35:12.713654Z","shell.execute_reply.started":"2021-12-05T23:35:12.701256Z","shell.execute_reply":"2021-12-05T23:35:12.712927Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"decoder = Sequential()\ndecoder.add(Conv2D(filters=3,kernel_size=(12,12),input_shape=(224,224,3),padding=\"same\", activation=LeakyReLU()))\n\ndecoder.add(Conv2D(filters=3,kernel_size=(12,12),padding=\"same\", activation=LeakyReLU()))\n\ndecoder.add(BatchNormalization())\ndecoder.add(Conv2D(filters=64,kernel_size=(12,12),padding=\"same\", activation=LeakyReLU()))\ndecoder.add(Conv2D(filters=64,kernel_size=(12,12),padding=\"same\", activation=LeakyReLU()))\n\n\ndecoder.add(BatchNormalization())\ndecoder.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\ndecoder.add(Conv2D(filters=128, kernel_size=(10,10), padding=\"same\", activation=LeakyReLU()))\ndecoder.add(Conv2D(filters=128, kernel_size=(10,10), padding=\"same\", activation=LeakyReLU()))\n\n\ndecoder.add(BatchNormalization())\ndecoder.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\ndecoder.add(Conv2D(filters=256, kernel_size=(8,8), padding=\"same\", activation=LeakyReLU()))\ndecoder.add(Conv2D(filters=256, kernel_size=(8,8), padding=\"same\", activation=LeakyReLU()))\n\n\ndecoder.add(BatchNormalization())\ndecoder.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\ndecoder.add(Conv2D(filters=512, kernel_size=(6,6), padding=\"same\", activation=LeakyReLU()))\ndecoder.add(Conv2D(filters=512, kernel_size=(6,6), padding=\"same\", activation=LeakyReLU()))\n\n\n#decoder.add(BatchNormalization())\n#decoder.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n#decoder.add(Conv2D(filters=512, kernel_size=(4,4), padding=\"same\", activation=LeakyReLU()))\n#decoder.add(Conv2D(filters=512, kernel_size=(4,4), padding=\"same\", activation=LeakyReLU()))\n\n\ndecoder.add(BatchNormalization())\n\ndecoder.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T23:35:17.312821Z","iopub.execute_input":"2021-12-05T23:35:17.313106Z","iopub.status.idle":"2021-12-05T23:35:19.842260Z","shell.execute_reply.started":"2021-12-05T23:35:17.313069Z","shell.execute_reply":"2021-12-05T23:35:19.841499Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"discriminator=Sequential()\ndiscriminator.add(VGG16(include_top=False,input_shape=(224,224,3)))\ndiscriminator.add(Flatten())\ndiscriminator.add(Dense(1,activation=\"sigmoid\"))\ndiscriminator.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T23:35:19.845230Z","iopub.execute_input":"2021-12-05T23:35:19.845430Z","iopub.status.idle":"2021-12-05T23:35:22.432059Z","shell.execute_reply.started":"2021-12-05T23:35:19.845405Z","shell.execute_reply":"2021-12-05T23:35:22.431256Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Compile both models in preparation for training\n\n\n# Build and compile the discriminator\n\ndiscriminator.compile(\n    loss='binary_crossentropy',\n    optimizer=Adam(0.0002, 0.5),\n    metrics=['accuracy'])\n\n# Build and compile the combined model\ngenerator = build_generator()\n\n# Create an input to represent noise sample from latent space\n#z = Input(shape=(latent_dim,))\n\n# Pass noise through generator to get an image\n#img = generator(z)\n\n# Make sure only the generator is trained\ndiscriminator.trainable = False\n\n\nautoencoder=Sequential()\nautoencoder.add(decoder)\nautoencoder.add(generator)\nprint(\"Here\")\nautoencoder.summary()\n\n# The true output is fake, but we label them real!\nfake_pred = Sequential()\nfake_pred.add(autoencoder)\nfake_pred.add(discriminator)\n\nfake_pred.summary()\n# Create the combined model object\n\n\n# Compile the combined model\nfake_pred.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))","metadata":{"execution":{"iopub.status.busy":"2021-12-05T23:35:28.402801Z","iopub.execute_input":"2021-12-05T23:35:28.403071Z","iopub.status.idle":"2021-12-05T23:35:29.000690Z","shell.execute_reply.started":"2021-12-05T23:35:28.403040Z","shell.execute_reply":"2021-12-05T23:35:28.999929Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Train the GAN\n\n\n# Config\nbatch_size = 64\nbatch=64\nepochs = 30000\nsample_period = 200 # every `sample_period` steps generate and save some data\n\n\n# Create batch labels to use when calling train_on_batch\nones = np.ones(batch_size)\nzeros = np.zeros(batch_size)\n\n# Store the losses\nd_losses = []\ng_losses = []\n\n# Create a folder to store generated images\nif not os.path.exists('gan_images'):\n  os.makedirs('gan_images')","metadata":{"execution":{"iopub.status.busy":"2021-12-05T23:52:09.497525Z","iopub.execute_input":"2021-12-05T23:52:09.497784Z","iopub.status.idle":"2021-12-05T23:52:09.503631Z","shell.execute_reply.started":"2021-12-05T23:52:09.497754Z","shell.execute_reply":"2021-12-05T23:52:09.502587Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def randImagePicker(batch):\n    img=[]\n    for x in range(batch):\n        pi=random.randint(0,ln-1)\n        pl=os.listdir(pathname+\"/\"+dirList[pi])\n        #print(pl)\n        pii=random.randint(0,len(pl)-1)\n        faceImg1=cv2.imread(pathname+\"/\"+dirList[pi]+\"/\"+pl[pii])\n        faceImg1=cv2.resize(faceImg1, (224, 224) ,interpolation = cv2.INTER_NEAREST)\n        \n        img.append(faceImg1)\n    return(np.array(img))\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-05T23:35:46.815664Z","iopub.execute_input":"2021-12-05T23:35:46.815929Z","iopub.status.idle":"2021-12-05T23:35:46.822753Z","shell.execute_reply.started":"2021-12-05T23:35:46.815899Z","shell.execute_reply":"2021-12-05T23:35:46.822047Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# A function to generate a grid of random samples from the generator\n# and save them to a file\ndef sample_images(epoch):\n  rows, cols = 5, 5\n  real_imgs=randImagePicker(rows*cols)\n  imgs = autoencoder.predict(real_imgs)\n  fig, axs = plt.subplots(rows, cols)\n  idx = 0\n  for i in range(rows):\n    for j in range(cols):\n      axs[i,j].imshow(imgs[idx])\n      axs[i,j].axis('off')\n      idx += 1\n  fig.savefig(\"%d.png\" % epoch)\n  plt.show()\n  plt.close()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T23:51:29.968296Z","iopub.execute_input":"2021-12-05T23:51:29.968905Z","iopub.status.idle":"2021-12-05T23:51:29.975656Z","shell.execute_reply.started":"2021-12-05T23:51:29.968862Z","shell.execute_reply":"2021-12-05T23:51:29.974792Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Main training loop\n\nfor epoch in range(epochs):\n  ###########################\n  ### Train discriminator ###\n  ###########################\n  \n  \n\n  ###########################\n  ###   Getting Images.   ###\n  ###########################\n  real_imgs=randImagePicker(batch_size)\n  #print(real_imgs.shape)\n  fake_imgs=autoencoder.predict(real_imgs)\n  #print(fake_imgs.shape)\n  #for i in fake_imgs:\n  #  plt.imshow(i)\n  #  plt.show()\n  \n\n\n  # Train the discriminator\n  # both loss and accuracy are returned\n  d_loss_real, d_acc_real = discriminator.train_on_batch(real_imgs, ones)\n  d_loss_fake, d_acc_fake = discriminator.train_on_batch(fake_imgs, zeros)\n  d_loss = 0.5 * (d_loss_real + d_loss_fake)\n  d_acc  = 0.5 * (d_acc_real + d_acc_fake)\n  \n  \n  #######################\n  ### Train generator ###\n  #######################\n  \n  \n  g_loss = fake_pred.train_on_batch(real_imgs, ones)\n  real_imgs=randImagePicker(batch_size)\n  g_loss = fake_pred.train_on_batch(real_imgs, ones)\n  \n  # Save the losses\n  d_losses.append(d_loss)\n  g_losses.append(g_loss)\n  \n  if epoch % 100 == 0:\n    print(f\"epoch: {epoch+1}/{epochs}, d_loss: {d_loss:.2f}, \\\n      d_acc: {d_acc:.2f}, g_loss: {g_loss:.2f}\")\n  \n  if epoch % sample_period == 0:\n    sample_images(epoch)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-05T23:52:24.503448Z","iopub.execute_input":"2021-12-05T23:52:24.504412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(g_losses, label='g_losses')\nplt.plot(d_losses, label='d_losses')\nplt.legend()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###Always save what you did forgot few timess!!!!!!!!!!\ndecoder.save('models/decoder')\ngenerator.save('model/generator')\ndiscriminator.save('model/discriminator')","metadata":{},"execution_count":null,"outputs":[]}]}